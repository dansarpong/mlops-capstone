[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
#
# Variable: AIRFLOW__CORE__DAGS_FOLDER
#
dags_folder = /opt/airflow/dags
# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package.function".
# 
# For example, default value ``airflow.utils.net.getfqdn`` means that result from patched
# version of `socket.getfqdn() <https://docs.python.org/3/library/socket.html#socket.getfqdn>`__,
# see related `CPython Issue <https://github.com/python/cpython/issues/49254>`__.
# 
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net.get_host_ip_address``
#
# Variable: AIRFLOW__CORE__HOSTNAME_CALLABLE
#
hostname_callable = airflow.utils.net.getfqdn
# A callable to check if a python file has airflow dags defined or not and should
# return ``True`` if it has dags otherwise ``False``.
# If this is not provided, Airflow uses its own heuristic rules.
# 
# The function should have the following signature
# 
# .. code-block:: python
# 
#     def func_name(file_path: str, zip_file: zipfile.ZipFile | None = None) -> bool: ...
#
# Variable: AIRFLOW__CORE__MIGHT_CONTAIN_DAG_CALLABLE
#
might_contain_dag_callable = airflow.utils.file.might_contain_dag_via_default_heuristic
# Default timezone in case supplied date times are naive
# can be `UTC` (default), `system`, or any `IANA <https://www.iana.org/time-zones>`
# timezone string (e.g. Europe/Amsterdam)
#
# Variable: AIRFLOW__CORE__DEFAULT_TIMEZONE
#
default_timezone = utc
# The executor class that airflow should use. Choices include
# ``LocalExecutor``, ``CeleryExecutor``,
# ``KubernetesExecutor`` or the full import path to the class when using a custom executor.
#
# Variable: AIRFLOW__CORE__EXECUTOR
#
executor = LocalExecutor
# The auth manager class that airflow should use. Full import path to the auth manager class.
#
# Variable: AIRFLOW__CORE__AUTH_MANAGER
#
auth_manager = airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
# The list of users and their associated role in simple auth manager. If the simple auth manager is
# used in your environment, this list controls who can access the environment.
# 
# List of user-role delimited with a comma. Each user-role is a colon delimited couple of username and
# role. Roles are predefined in simple auth managers: viewer, user, op, admin.
#
# Example: simple_auth_manager_users = bob:admin,peter:viewer
#
# Variable: AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS
#
simple_auth_manager_users = admin:admin
# Whether to disable authentication and allow everyone as admin in the environment.
#
# Variable: AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS
#
simple_auth_manager_all_admins = False
# The json file where the simple auth manager stores passwords for the configured users.
# By default this is ``AIRFLOW_HOME/simple_auth_manager_passwords.json.generated``.
#
# Example: simple_auth_manager_passwords_file = /path/to/passwords.json
#
# Variable: AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE
#
# simple_auth_manager_passwords_file = 
# This defines the maximum number of task instances that can run concurrently per scheduler in
# Airflow, regardless of the worker count. Generally this value, multiplied by the number of
# schedulers in your cluster, is the maximum number of task instances with the running
# state in the metadata database. The value must be larger or equal 1.
#
# Variable: AIRFLOW__CORE__PARALLELISM
#
parallelism = 32
# The maximum number of task instances allowed to run concurrently in each DAG. To calculate
# the number of tasks that is running concurrently for a DAG, add up the number of running
# tasks for all DAG runs of the DAG. This is configurable at the DAG level with ``max_active_tasks``,
# which is defaulted as ``[core] max_active_tasks_per_dag``.
# 
# An example scenario when this would be useful is when you want to stop a new dag with an early
# start date from stealing all the executor slots in a cluster.
#
# Variable: AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG
#
max_active_tasks_per_dag = 16
# Are DAGs paused by default at creation
#
# Variable: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
#
dags_are_paused_at_creation = True
# The maximum number of active DAG runs per DAG. The scheduler will not create more DAG runs
# if it reaches the limit. This is configurable at the DAG level with ``max_active_runs``,
# which is defaulted as ``[core] max_active_runs_per_dag``.
#
# Variable: AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG
#
max_active_runs_per_dag = 16
# (experimental) The maximum number of consecutive DAG failures before DAG is automatically paused.
# This is also configurable per DAG level with ``max_consecutive_failed_dag_runs``,
# which is defaulted as ``[core] max_consecutive_failed_dag_runs_per_dag``.
# If not specified, then the value is considered as 0,
# meaning that the dags are never paused out by default.
#
# Variable: AIRFLOW__CORE__MAX_CONSECUTIVE_FAILED_DAG_RUNS_PER_DAG
#
max_consecutive_failed_dag_runs_per_dag = 0
# The name of the method used in order to start Python processes via the multiprocessing module.
# This corresponds directly with the options available in the Python docs:
# `multiprocessing.set_start_method
# <https://docs.python.org/3/library/multiprocessing.html#multiprocessing.set_start_method>`__
# must be one of the values returned by `multiprocessing.get_all_start_methods()
# <https://docs.python.org/3/library/multiprocessing.html#multiprocessing.get_all_start_methods>`__.
#
# Example: mp_start_method = fork
#
# Variable: AIRFLOW__CORE__MP_START_METHOD
#
# mp_start_method = 
# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to ``False`` in a production
# environment
#
# Variable: AIRFLOW__CORE__LOAD_EXAMPLES
#
load_examples = True
# Path to the folder containing Airflow plugins
#
# Variable: AIRFLOW__CORE__PLUGINS_FOLDER
#
plugins_folder = /opt/airflow/plugins
# Should tasks be executed via forking of the parent process
# 
# * ``False``: Execute via forking of the parent process
# * ``True``: Spawning a new python process, slower than fork, but means plugin changes picked
#   up by tasks straight away
#
# Variable: AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER
#
execute_tasks_new_python_interpreter = False
# Secret key to save connection passwords in the db
#
# Variable: AIRFLOW__CORE__FERNET_KEY
#
fernet_key = yKEU_II5yJSD_4w3PTnhSe53F7K0VEin_-v7M6Ew5FQ=
# Whether to disable pickling dags
#
# Variable: AIRFLOW__CORE__DONOT_PICKLE
#
donot_pickle = True
# How long before timing out a python file import
#
# Variable: AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT
#
dagbag_import_timeout = 30.0
# Should a traceback be shown in the UI for dagbag import errors,
# instead of just the exception message
#
# Variable: AIRFLOW__CORE__DAGBAG_IMPORT_ERROR_TRACEBACKS
#
dagbag_import_error_tracebacks = True
# If tracebacks are shown, how many entries from the traceback should be shown
#
# Variable: AIRFLOW__CORE__DAGBAG_IMPORT_ERROR_TRACEBACK_DEPTH
#
dagbag_import_error_traceback_depth = 2
# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
#
# Variable: AIRFLOW__CORE__DEFAULT_IMPERSONATION
#
default_impersonation = 
# What security module to use (for example kerberos)
#
# Variable: AIRFLOW__CORE__SECURITY
#
security = 
# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
#
# Variable: AIRFLOW__CORE__UNIT_TEST_MODE
#
unit_test_mode = False
# Space-separated list of classes that may be imported during deserialization. Items can be glob
# expressions. Python built-in classes (like dict) are always allowed.
#
# Example: allowed_deserialization_classes = airflow.* my_mod.my_other_mod.TheseClasses*
#
# Variable: AIRFLOW__CORE__ALLOWED_DESERIALIZATION_CLASSES
#
allowed_deserialization_classes = airflow.*
# Space-separated list of classes that may be imported during deserialization. Items are processed
# as regex expressions. Python built-in classes (like dict) are always allowed.
# This is a secondary option to ``[core] allowed_deserialization_classes``.
#
# Variable: AIRFLOW__CORE__ALLOWED_DESERIALIZATION_CLASSES_REGEXP
#
allowed_deserialization_classes_regexp = 
# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
#
# Variable: AIRFLOW__CORE__KILLED_TASK_CLEANUP_TIME
#
killed_task_cleanup_time = 60
# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
#
# Variable: AIRFLOW__CORE__DAG_RUN_CONF_OVERRIDES_PARAMS
#
dag_run_conf_overrides_params = True
# If enabled, Airflow will only scan files containing both ``DAG`` and ``airflow`` (case-insensitive).
#
# Variable: AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE
#
dag_discovery_safe_mode = True
# The pattern syntax used in the
# `.airflowignore
# <https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#airflowignore>`__
# files in the DAG directories. Valid values are ``regexp`` or ``glob``.
#
# Variable: AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX
#
dag_ignore_file_syntax = glob
# The number of retries each task is going to have by default. Can be overridden at dag or task level.
#
# Variable: AIRFLOW__CORE__DEFAULT_TASK_RETRIES
#
default_task_retries = 0
# The number of seconds each task is going to wait by default between retries. Can be overridden at
# dag or task level.
#
# Variable: AIRFLOW__CORE__DEFAULT_TASK_RETRY_DELAY
#
default_task_retry_delay = 300
# The maximum delay (in seconds) each task is going to wait by default between retries.
# This is a global setting and cannot be overridden at task or DAG level.
#
# Variable: AIRFLOW__CORE__MAX_TASK_RETRY_DELAY
#
max_task_retry_delay = 86400
# The weighting method used for the effective total priority weight of the task
#
# Variable: AIRFLOW__CORE__DEFAULT_TASK_WEIGHT_RULE
#
default_task_weight_rule = downstream
# Maximum possible time (in seconds) that task will have for execution of auxiliary processes
# (like listeners, mini scheduler...) after task is marked as success..
#
# Variable: AIRFLOW__CORE__TASK_SUCCESS_OVERTIME
#
task_success_overtime = 20
# The default task execution_timeout value for the operators. Expected an integer value to
# be passed into timedelta as seconds. If not specified, then the value is considered as None,
# meaning that the operators are never timed out by default.
#
# Variable: AIRFLOW__CORE__DEFAULT_TASK_EXECUTION_TIMEOUT
#
default_task_execution_timeout = 
# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
#
# Variable: AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL
#
min_serialized_dag_update_interval = 30
# If ``True``, serialized DAGs are compressed before writing to DB.
# 
# .. note::
# 
#     This will disable the DAG dependencies view
#
# Variable: AIRFLOW__CORE__COMPRESS_SERIALIZED_DAGS
#
compress_serialized_dags = False
# Fetching serialized DAG can not be faster than a minimum interval to reduce database
# read rate. This config controls when your DAGs are updated in the Webserver
#
# Variable: AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL
#
min_serialized_dag_fetch_interval = 10
# Maximum number of Rendered Task Instance Fields (Template Fields) per task to store
# in the Database.
# All the template_fields for each of Task Instance are stored in the Database.
# Keeping this number small may cause an error when you try to view ``Rendered`` tab in
# TaskInstance view for older tasks.
#
# Variable: AIRFLOW__CORE__MAX_NUM_RENDERED_TI_FIELDS_PER_TASK
#
max_num_rendered_ti_fields_per_task = 30
# Path to custom XCom class that will be used to store and resolve operators results
#
# Example: xcom_backend = path.to.CustomXCom
#
# Variable: AIRFLOW__CORE__XCOM_BACKEND
#
xcom_backend = airflow.sdk.execution_time.xcom.BaseXCom
# By default Airflow plugins are lazily-loaded (only loaded when required). Set it to ``False``,
# if you want to load plugins whenever 'airflow' is invoked via cli or loaded from module.
#
# Variable: AIRFLOW__CORE__LAZY_LOAD_PLUGINS
#
lazy_load_plugins = True
# By default Airflow providers are lazily-discovered (discovery and imports happen only when required).
# Set it to ``False``, if you want to discover providers whenever 'airflow' is invoked via cl...
