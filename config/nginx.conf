events {
    worker_connections 1024;
}

http {
    upstream inference_backend {
        # For A/B testing, we can use different weights
        server inference-api:8000 weight=50;
        # server inference-api-b:8000 weight=50;  # Second model instance
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/m;

    server {
        listen 80;
        server_name localhost;

        # Health check endpoint
        location /health {
            proxy_pass http://inference_backend/health;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # Main API endpoints with rate limiting
        location / {
            limit_req zone=api_limit burst=20 nodelay;
            
            proxy_pass http://inference_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeout settings
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # Add custom headers for A/B testing
            add_header X-Model-Version $upstream_addr always;
        }

        # Metrics endpoint (restricted access)
        location /metrics {
            # allow 10.0.0.0/8;
            # deny all;
            
            proxy_pass http://inference_backend/metrics;
            proxy_set_header Host $host;
        }

        # Static health check for load balancer
        location /nginx-health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }

    # Logging
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;
}
